{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nnsight\n",
    "from PIL import Image\n",
    "import os, json, argparse, itertools, math, torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "from vllm.inputs import TextPrompt\n",
    "from nnsight.modeling.vllm import VLLM\n",
    "import einops\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-04 13:17:09 config.py:2167] Downcasting torch.float32 to torch.float16.\n",
      "INFO 05-04 13:17:09 config.py:478] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-04 13:17:09 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='gpt2', speculative_config=None, tokenizer='gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=gpt2, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-04 13:17:10 model_runner.py:1092] Starting to load model gpt2...\n",
      "INFO 05-04 13:17:10 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 05-04 13:17:10 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05865d7b2564e0bad74a542e2f221fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-04 13:17:11 model_runner.py:1097] Loading model weights took 0.2460 GB\n",
      "INFO 05-04 13:17:12 worker.py:241] Memory profiling takes 0.47 seconds\n",
      "INFO 05-04 13:17:12 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.90) = 125.75GiB\n",
      "INFO 05-04 13:17:12 worker.py:241] model weights take 0.25GiB; non_torch_memory takes -0.02GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 125.06GiB.\n",
      "INFO 05-04 13:17:12 gpu_executor.py:76] # GPU blocks: 227659, # CPU blocks: 7281\n",
      "INFO 05-04 13:17:12 gpu_executor.py:80] Maximum concurrency for 1024 tokens per request: 3557.17x\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12524/3361498698.py\", line 1, in <module>\n",
      "    vllm = VLLM(\"gpt2\", device = \"auto\", dispatch = True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/vllm/vllm.py\", line 68, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/mixins/meta.py\", line 22, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/mixins/loadable.py\", line 14, in __init__\n",
      "    model = self._load(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/vllm/vllm.py\", line 141, in _load\n",
      "    llm = LLM(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 990, in inner\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 230, in __init__\n",
      "    # compare class name. Misjudgment will arise from\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 532, in from_engine_args\n",
      "    if distributed_executor_backend == \"ray\":\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 291, in __init__\n",
      "    model_config.seed,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 444, in _initialize_kv_caches\n",
      "    self.stat_loggers = {\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 83, in initialize_cache\n",
      "    worker_class_fn) = self._get_worker_module_and_class()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 274, in initialize_cache\n",
      "    self.cache_config.num_cpu_blocks = num_cpu_blocks\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 279, in _init_cache_engine\n",
      "    def _init_cache_engine(self):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 280, in <listcomp>\n",
      "    assert self.cache_config.num_gpu_blocks is not None\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/cache_engine.py\", line 62, in __init__\n",
      "    self.gpu_cache = self._allocate_kv_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/cache_engine.py\", line 81, in _allocate_kv_cache\n",
      "    torch.zeros(kv_cache_shape,\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.42 GiB. GPU 0 has a total capacity of 139.72 GiB of which 3.05 GiB is free. Process 3531337 has 136.65 GiB memory in use. Of the allocated memory 135.85 GiB is allocated by PyTorch, with 26.00 MiB allocated in private pools (e.g., CUDA Graphs), and 20.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "vllm = VLLM(\"gpt2\", device = \"auto\", dispatch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 283.31it/s, est. speed input: 3133.64 toks/s, output: 284.72 toks/s]\n"
     ]
    }
   ],
   "source": [
    "with vllm.trace(temperature=0.0, top_p=1.0, max_tokens=1) as tracer:\n",
    "  with tracer.invoke(\"The Eiffel Tower is located in the city of\"):\n",
    "    clean_logits = vllm.logits.output.save()\n",
    "\n",
    "  with tracer.invoke(\"The Eiffel Tower is located in the city of\"):\n",
    "    vllm.transformer.h[-2].mlp.output[:] = 0\n",
    "    corrupted_logits = vllm.logits.output.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLEAN - The Eiffel Tower is located in the city of  Paris\n",
      "\n",
      "CORRUPTED - The Eiffel Tower is located in the city of  London\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCLEAN - The Eiffel Tower is located in the city of\", vllm.tokenizer.decode(clean_logits.argmax(dim=-1)))\n",
    "print(\"\\nCORRUPTED - The Eiffel Tower is located in the city of\", vllm.tokenizer.decode(corrupted_logits.argmax(dim=-1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

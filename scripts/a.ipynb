{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nnsight\n",
    "from PIL import Image\n",
    "import os, json, argparse, itertools, math, torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "from vllm.inputs import TextPrompt\n",
    "from nnsight.modeling.vllm import VLLM\n",
    "import einops\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 20:04:36 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='llava-hf/llava-1.5-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-1.5-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=llava-hf/llava-1.5-7b-hf, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 05-05 20:04:36 selector.py:135] Using Flash Attention backend.\n",
      "INFO 05-05 20:04:36 model_runner.py:1072] Starting to load model llava-hf/llava-1.5-7b-hf...\n",
      "INFO 05-05 20:04:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5129499f804dc1b7fb1e7d306c4dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 20:04:39 model_runner.py:1077] Loading model weights took 13.1342 GB\n",
      "INFO 05-05 20:04:40 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=13.77GiB peak_torch_memory=13.64GiB memory_usage_post_profile=13.88GiB non_torch_memory=0.71GiB kv_cache_size=111.39GiB gpu_memory_utilization=0.90\n",
      "INFO 05-05 20:04:40 gpu_executor.py:113] # GPU blocks: 14257, # CPU blocks: 512\n",
      "INFO 05-05 20:04:40 gpu_executor.py:117] Maximum concurrency for 4096 tokens per request: 55.69x\n",
      "INFO 05-05 20:04:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-05 20:04:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-05 20:04:48 model_runner.py:1518] Graph capturing finished in 6 secs, took 0.30 GiB\n",
      "QKVParallelLinear(in_features=4096, output_features=12288, bias=False, tp_size=1, gather_output=False)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = VLLM(model_name, device='cuda', dispatch=True)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(model.language_model.model.layers[0].self_attn.qkv_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.language_model.model.layers[0].self_attn.o_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s, est. speed input: 8.25 toks/s, output: 66.01 toks/s]\n"
     ]
    }
   ],
   "source": [
    "with model.trace() as tr, tr.invoke(TextPrompt(prompt=\"Hello\", multi_modal_data={'image': Image.open(\"test.png\")})):\n",
    "    qkv = model.language_model.model.layers[0].self_attn.qkv_proj.output.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 8.8196e-02, -2.7979e-01,  5.8545e-01,  ..., -5.6763e-03,\n",
      "         -2.1347e-02,  3.1018e-04],\n",
      "        [ 1.6289e+00, -2.1094e+00,  9.0576e-01,  ...,  4.6349e-03,\n",
      "         -8.3237e-03,  8.4000e-03]], device='cuda:0', dtype=torch.float16), None)\n"
     ]
    }
   ],
   "source": [
    "print(qkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 20:08:15 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
      "WARNING 05-05 20:08:15 utils.py:445] The number of multi-modal placeholder tokens in the prompt is less than the number of multi-modal inputs. Extra placeholder tokens will be treated as plain text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "NNsightError",
     "evalue": "Multi-modal placeholders and items must have the same length.",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/graph/node.py\", line 289, in execute",
      "    self.target.execute(self)",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/contexts/interleaving.py\", line 161, in execute",
      "    graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/vllm/vllm.py\", line 252, in interleave",
      "    return fn(*args, **kwargs)",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/vllm/vllm.py\", line 269, in _execute",
      "    self.vllm_entrypoint.generate(prompts, sampling_params=params)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 1063, in inner",
      "    return fn(*args, **kwargs)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 406, in generate",
      "    outputs = self._run_engine(use_tqdm=use_tqdm)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 942, in _run_engine",
      "    step_outputs = self.llm_engine.step()",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 1454, in step",
      "    outputs = self.model_executor.execute_model(",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 125, in execute_model",
      "    output = self.driver_worker.execute_model(execute_model_req)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 319, in execute_model",
      "    inputs = self.prepare_input(execute_model_req)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 307, in prepare_input",
      "    return self._get_driver_input_and_broadcast(execute_model_req)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 269, in _get_driver_input_and_broadcast",
      "    self.model_runner.prepare_model_input(",
      "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/vllm/model_runners/GPUModelRunner.py\", line 139, in prepare_model_input",
      "    model_input = self._prepare_model_input_tensors(",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1202, in _prepare_model_input_tensors",
      "    builder.add_seq_group(seq_group_metadata)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 740, in add_seq_group",
      "    per_seq_group_fn(inter_data, seq_group_metadata)",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 658, in _compute_multi_modal_input",
      "    mm_data, placeholder_maps = MultiModalPlaceholderMap.from_seq_group(",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/base.py\", line 347, in from_seq_group",
      "    modality].append_items_from_seq_group(",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/base.py\", line 368, in append_items_from_seq_group",
      "    raise ValueError(",
      "ValueError: Multi-modal placeholders and items must have the same length.",
      "",
      "During handling of the above exception, another exception occurred:",
      "",
      "Traceback (most recent call last):",
      "  File \"/tmp/ipykernel_4791/1143143481.py\", line 2, in <module>",
      "    with model.trace() as tr, tr.invoke(TextPrompt(prompt=\"Hello\", multi_modal_data={'image': img})):",
      "",
      "NNsightError: Multi-modal placeholders and items must have the same length."
     ]
    }
   ],
   "source": [
    "img = Image.open(\"/root/multimodal-interpretability/scripts/dataset/clean-1.jpg\")\n",
    "with model.trace() as tr, tr.invoke(TextPrompt(prompt=\"Hello\", multi_modal_data={'image': img})):\n",
    "    qkv = model.language_model.model.layers[0].self_attn.qkv_proj.output.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
